# Data Analysis

## Introduction

Data analysis plays a quintessential role in the realm of communication and media research. With an ever-increasing volume of data generated from diverse sources, researchers need robust techniques to sift through this raw information and discern meaningful patterns or trends. This section outlines the objectives of data analysis, categorizes the major types of statistical analyses commonly used, and discusses the pivotal role of statistical software such as RStudio in facilitating this analytical process.

### Objectives and Importance of Data Analysis {.unnumbered}

The fundamental aim of data analysis is to distill large amounts of information into actionable insights. In the context of communication and media research, these objectives can be further refined as follows:

**Summarization**: To present data in a digestible format, offering a clear snapshot of its main features. **Exploration**: To identify relationships or trends within the data, providing a basis for further investigation. **Inference**: To make educated guesses or predictions about a broader population based on sample data. **Validation**: To confirm or negate existing theories or hypotheses through empirical evidence. **Decision-Making**: To provide actionable recommendations and insights that may influence policy, strategy, or further academic research.

Ignoring proper data analysis can lead to misleading conclusions or partial understandings, affecting the quality and reliability of the research. Thus, the importance of meticulous data analysis cannot be overstated.

### Types of Analyses: Descriptive vs. Inferential {.unnumbered}

#### Descriptive Analysis {.unnumbered}

In descriptive statistics, the aim is to summarize the main aspects of the data in hand, often through tables, graphs, or numerical measures such as mean, median, and standard deviation. Descriptive analysis provides a compact representation of the data, but it does not allow researchers to make conclusions beyond the data at hand (Tukey, 1977).

#### Inferential Analysis {.unnumbered}

In contrast, inferential statistics go a step further by enabling researchers to draw conclusions about a population based on a sample. Inferential methods like t-tests, ANOVA, and regressions allow one to assess hypotheses and derive estimates that are generalizable to a broader context (Cohen, 1988).

### Role of Statistical Software: E.g., RStudio {.unnumbered}

In the current digital age, statistical software has become an indispensable tool for data analysis. RStudio is one such environment that offers a wide array of statistical and graphical techniques. It is especially favored for its:

1.  **User-Friendly Interface**: RStudio provides a clean and efficient interface for executing R code, thereby easing the process of data analysis.
2.  **Flexibility and Adaptability**: It supports various data formats and can be integrated with other software and programming languages.
3.  **Extensive Libraries**: With a rich ecosystem of packages like `ggplot2` for data visualization, `dplyr` for data manipulation, and `caret` for machine learning, RStudio offers comprehensive analytical capabilities.
4.  **Reproducibility**: The code-based nature of RStudio ensures that analyses can be easily documented and reproduced, adhering to the tenets of reliable scientific research (Peng, 2011).

By mastering RStudio or similar statistical software, researchers are better equipped to conduct complex analyses that can contribute to robust and insightful findings.

## Descriptive Analysis

Descriptive statistics form the bedrock of data exploration and initial data analysis. These statistics facilitate the comprehensive summarization, condensation, and general understanding of the structural attributes of expansive datasets (De Veaux, Velleman, & Bock, 2018). Employed as a precursor to more advanced statistical procedures, descriptive statistics offer a straightforward way to describe the main aspects of a data set, from the typical values to the variability within the set. They provide researchers with tools to quickly identify patterns, trends, and potential outliers without making generalized predictions about larger populations (Boslaugh, 2012). Furthermore, descriptive statistics are essential in exploratory data analysis, where their role is to aid in the detection of any unusual observations that may warrant further investigation (Tukey, 1977).

Moreover, descriptive statistics have applications that span across various domains---from social sciences to economics, from healthcare to engineering. The utility lies in their ability to translate large amounts of data into easily understandable formats, such as graphs, tables, and numerical measures, thereby transforming raw data into insightful information. In research, they often serve as the initial step in the process of data analytics, giving researchers a snapshot of what the data looks like before delving into more complex analytical techniques like inferential statistics or machine learning algorithms (Hair et al., 2014).

### Measures of Central Tendency {.unnumbered}

```{r}
# Load the packages
library(tidyverse)
library(data.table)

# Import the datasets
spotify_songs <- fread("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv")
movies <- fread("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-09/movies.csv")

# Preview the first few rows of the datasets
head(spotify_songs)
head(movies)
```

#### Mean {.unnumbered}

The mean is perhaps the most widely recognized measure of central tendency, representing the arithmetic average of a dataset. To compute the mean, all the data points are summed up and then divided by the number of data points (Lind, Marchal, & Wathen, 2012). The mean is sensitive to outliers, which can disproportionately influence the calculated average, potentially resulting in a misleading representation of central location (McClave, Benson, & Sincich, 2011). Despite this limitation, the mean is highly useful in various statistical methods, including regression analysis and hypothesis testing, because of its mathematical properties (Field, Miles, & Field, 2012).

Importantly, the mean can be categorized into different types: arithmetic mean, geometric mean, and harmonic mean, each with specific applications depending on the nature of the data and the intended analysis (Triola, 2018). For instance, the geometric mean is often used when dealing with data that exhibit exponential growth or decline, such as in financial or biological contexts (Cox & Snell, 1981).

**Example using Spotify Songs Dataset**: To find the mean popularity of songs.

```{r}
mean_popularity <- spotify_songs %>% 
  summarise(mean_popularity = mean(track_popularity, na.rm = TRUE))

mean_popularity
```

#### Median {.unnumbered}

The median serves as another measure of central tendency and is less sensitive to outliers compared to the mean (Lind et al., 2012). It is defined as the middle value in a dataset that has been arranged in ascending order. If the dataset contains an even number of observations, the median is calculated as the average of the two middle numbers. Medians are particularly useful for data that are skewed or contain outliers, as they provide a more "resistant" measure of the data's central location (Hoaglin, Mosteller, & Tukey, 2000).

In addition to its robustness against outliers, the median is often used in non-parametric statistical tests like the Mann-Whitney U test and the Kruskal-Wallis test. These tests do not assume that the data follow a specific distribution, making the median an invaluable asset in such scenarios (Siegel & Castellan, 1988).

**Example using Movies Dataset**: To find the median budget of movies.

```{r}
median_budget <- movies %>% 
  summarise(median_budget = median(budget, na.rm = TRUE))

median_budget
```

#### Mode {.unnumbered}

The mode refers to the value or values that appear most frequently in a dataset (Gravetter & Wallnau, 2016). A dataset can be unimodal, having one mode; bimodal, having two modes; or multimodal, having multiple modes. While the mode is less commonly used than the mean and median for numerical data, it is the primary measure of central tendency for categorical or nominal data (Agresti, 2002).

Despite its less frequent application in numerical contexts, the mode can still be useful for identifying the most common values in a dataset and for understanding the general distribution of the data (Bland & Altman, 1996). For example, in market research, knowing the mode of a dataset related to consumer preferences can provide valuable insights into what most consumers are likely to choose.

**Example using Spotify Songs Dataset**: To find the mode of the `playlist_genre`.

```{r}
mode_genre <- spotify_songs %>% 
  count(playlist_genre) %>% 
  arrange(desc(n)) %>% 
  slice_head(n = 1)

mode_genre
```

### Measures of Dispersion {.unnumbered}

#### Range {.unnumbered}

The range is the simplest measure of dispersion, calculated by subtracting the smallest value from the largest value in the dataset (McClave, Benson, & Sincich, 2011). While straightforward to compute, the range is highly sensitive to outliers and does not account for how the rest of the values in the dataset are distributed (Triola, 2018).

The range offers a quick, albeit crude, estimate of the dataset's variability. It is often used in conjunction with other measures of dispersion for a more comprehensive understanding of data spread. Despite its limitations, the range can be helpful in initial exploratory analyses to quickly identify the scope of the data and to detect possible outliers or data entry errors (Tukey, 1977).

**Example using Movies Dataset**: To find the range of movie budgets.

```{r}
budget_range <- movies %>% 
  summarise(Range = max(budget, na.rm = TRUE) - min(budget, na.rm = TRUE))

budget_range
```

#### Standard Deviation {.unnumbered}

The standard deviation is a more sophisticated measure of dispersion that indicates how much individual data points deviate from the mean (Lind et al., 2012). Calculated as the square root of the variance, the standard deviation provides an intuitive sense of the data's spread since it is in the same unit as the original data points. It plays a crucial role in various statistical analyses, including hypothesis testing and confidence interval estimation, and is fundamental in fields ranging from finance to natural sciences (Levine, Stephan, Krehbiel, & Berenson, 2008).

The standard deviation can be classified into two types: population standard deviation and sample standard deviation. The former is used when the data represent an entire population, while the latter is used for sample data and is calculated with a slight adjustment to account for sample bias (Kenney & Keeping, 1962).

**Example using Spotify Songs Dataset**: To find the standard deviation of `danceability`.

```{r}
std_danceability <- spotify_songs %>% 
  summarise(std_danceability = sd(danceability, na.rm = TRUE))

std_danceability
```

#### Variance {.unnumbered}

Variance is closely related to the standard deviation, essentially being its square. It quantifies how much individual data points in a dataset differ from the mean (Gravetter & Wallnau, 2016). Unlike the standard deviation, the variance is not in the same unit as the data, which can make it less intuitive to interpret. However, variance has essential mathematical properties that make it useful in statistical modeling and hypothesis testing (Moore, McCabe, & Craig, 2009).

In statistical theory, the concept of variance is pivotal for various analytical techniques, such as Analysis of Variance (ANOVA) and Principal Component Analysis (PCA). Variance allows for the decomposition of data into explained and unexplained components, serving as a key element in understanding data variability in greater depth (Johnson & Wichern, 2007).

**Example using Movies Dataset**: To find the variance in IMDB ratings.

```{r}
var_imdb_rating <- movies %>% 
  summarise(var_imdb_rating = var(imdb_rating, na.rm = TRUE))

var_imdb_rating
```

## Inferential Analysis {.unnumbered}

Inferential statistics serve as the cornerstone for drawing conclusions about a population based on a sample. Unlike descriptive statistics, which aim to summarize data, inferential statistics allow for hypothesis testing, predictions, and inferences about the data (Field, Miles, & Field, 2012). The utility of inferential statistics lies in its ability to generalize findings beyond the immediate data to broader contexts. This is particularly valuable in research areas where it's impractical to collect data from an entire population (Frankfort-Nachmias, Leon-Guerrero, & Davis, 2020).

The application of inferential statistics often involves the use of various tests and models to determine statistical significance, which in turn helps researchers make meaningful inferences. Such analyses are commonly used in disciplines like psychology, economics, and medicine, to name a few. They provide a quantitative basis for conclusions and decisions, which is fundamental for scientific research (Rosner, 2015). Given the capacity to test theories and hypotheses, inferential statistics remain an indispensable tool in the scientific community.

### Comparison of Means {.unnumbered}

#### T-test {.unnumbered}

The T-test is a statistical method used to determine if there is a significant difference between the means of two groups. It is commonly used to compare two samples to determine if they could have originated from the same population (Rosner, 2015). The T-test operates under certain assumptions, such as the data being normally distributed and the samples being independent of each other. Violation of these assumptions may lead to misleading results.

**Example with `movies` dataset:**

```{r}
# Calculate the mean budget for action and drama movies
action_movies <- movies %>% filter(genre == 'Action')
drama_movies <- movies %>% filter(genre == 'Drama')

# Perform t-test
t.test(action_movies$budget, drama_movies$budget)
```

**Independent Sample T-test**

An independent sample T-test is used when comparing the means of two independent groups to assess whether their means are statistically different (Field et al., 2012). The groups should be separate, meaning the performance or attributes of one group should not influence the other. For instance, this type of T-test might be used to compare the average test scores of two different classrooms. It's essential to note that both groups should be normally distributed, and ideally, they should have the same variance for the T-test to be applicable.

**Example with `Survivor summary.csv` and `viewers.csv`:**

```{r}
# Load data
summary <- fread("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-06-01/summary.csv")

# Compare average viewers for seasons in different locations
fiji_seasons <- summary %>% filter(country == 'Fiji')
other_seasons <- summary %>% filter(country != 'Fiji')

# Perform t-test
t.test(fiji_seasons$viewers_mean, other_seasons$viewers_mean)
```

**Paired Sample T-test**

In contrast, a paired sample T-test is designed to compare means from the same group at different times or under different conditions (Vasishth & Broe, 2011). For example, it could be used to compare student test scores before and after a training program. Here, the assumption is that the differences between pairs follow a normal distribution. Paired T-tests are particularly useful in "before and after" scenarios, where each subject serves as their control, thereby increasing the test's sensitivity.

**Example with Survivor's `summary.csv`:**

```{r}

# Perform paired t-test to compare viewership at premier and finale
paired_t_test_result <- t.test(summary$viewers_premier, summary$viewers_finale, paired = TRUE)

# Output the result
paired_t_test_result

```

#### Analysis of Variance (ANOVA) {.unnumbered}

ANOVA is a more generalized form of the T-test and is used when there are more than two groups to compare (Kutner, Nachtsheim, & Neter, 2004). The underlying principle of ANOVA is to partition the variance within the data into "between-group" and "within-group" variance, to identify any significant differences in means.

**One-way ANOVA**

One-way ANOVA focuses on a single independent variable with more than two levels or groups (Tabachnick & Fidell, 2013). It allows researchers to test if there are statistically significant differences between the means of three or more independent groups. It is widely used in various fields, including psychology, business, and healthcare, for testing the impact of different conditions or treatments.

**Example with `Survivor castaways.csv`:**

```{r}
castaways <- fread("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-06-01/castaways.csv")


# Perform one-way ANOVA for total_votes_received among different personality types
anova_result <- aov(total_votes_received ~ personality_type, data = castaways)
summary(anova_result)
```

**Two-way ANOVA**

Two-way ANOVA, however, involves two independent variables, offering a more intricate comparison and understanding of the interaction effects (Winer, Brown, & Michels, 1991). It helps to analyze how two factors impact a dependent variable, and it can also show how the two independent variables interact with each other. This form of ANOVA is highly valuable in experimental design where multiple variables may influence the outcome.

**Example with `movies` dataset:**

```{r}
# Perform two-way ANOVA for budget by genre and year
anova_result <- aov(budget ~ genre * year, data = movies)
summary(anova_result)
```

------------------------------------------------------------------------

### Regression Analysis {.unnumbered}

#### Simple Linear Regression {.unnumbered}

Simple linear regression aims to model the relationship between a single independent variable and a dependent variable by fitting a linear equation to observed data (Montgomery, Peck, & Vining, 2012). The primary objective is to find the best-fitting straight line that accurately predicts the output values within a range. Simple linear regression works best when the variables have a linear relationship, and the data is homoscedastic, meaning the variance of errors is constant across levels of the independent variable.

**Example with `Survivor viewers.csv`:**

```{r}
viewers <- fread("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-06-01/viewers.csv")

# Model viewers based on episode numbers
lm_result <- lm(viewers ~ episode, data = viewers)
summary(lm_result)
```

#### Multiple Linear Regression {.unnumbered}

Multiple linear regression extends the concept of simple linear regression to include two or more independent variables (Hair et al., 2014). This approach allows for a more nuanced understanding of the relationships among variables. It provides the tools needed to predict a dependent variable based on the values of multiple independent variables. Multiple linear regression assumes that the relationship between the dependent variable and the independent variables is linear, and it also assumes that the residuals are normally distributed and have constant variance.

**Example with `Survivor summary.csv`:**

```{r}
# Model average viewers based on multiple factors
lm_result <- lm(viewers_mean ~ country + timeslot + season, data = summary)
summary(lm_result)
```

### References

-   Agresti, A. (2002). Categorical Data Analysis. John Wiley & Sons.
-   Bland, J. M., & Altman, D. G. (1996). Statistics notes: Transforming data. BMJ, 312(7033), 770.
-   Boslaugh, S. (2012). Statistics in a Nutshell: A Desktop Quick Reference. O'Reilly Media.
-   Cox, D. R., & Snell, E. J. (1981). Applied statistics: principles and examples. Chapman and Hall.
-   De Veaux, R. D., Velleman, P. F., & Bock, D. E. (2018). Stats: Data and models (4th ed.). Pearson.
-   Field, A. P., Miles, J., & Field, Z. (2012). Discovering statistics using R. Sage publications.
-   Frankfort-Nachmias, C., Leon-Guerrero, A., & Davis, G. (2020). Social statistics for a diverse society. Sage Publications.
-   Gravetter, F. J., & Wallnau, L. B. (2016). Essentials of statistics for the behavioral sciences (8th ed.). Wadsworth Cengage Learning.
-   Hair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2014). Multivariate Data Analysis. Pearson Education Limited.
-   Hoaglin, D. C., Mosteller, F., & Tukey, J. W. (2000). Understanding Robust and Exploratory Data Analysis. John Wiley & Sons.
-   Johnson, R. A., & Wichern, D. W. (2007). Applied Multivariate Statistical Analysis. Pearson Prentice Hall.
-   Kenney, J. F., & Keeping, E. S. (1962). Mathematics of Statistics, Pt. 2, 2nd ed. Van Nostrand.
-   Kutner, M. H., Nachtsheim, C. J., & Neter, J. (2004). Applied linear regression models. McGraw-Hill Irwin.
-   Levine, D. M., Stephan, D. F., Krehbiel, T. C., & Berenson, M. L. (2008). Statistics for Managers Using Microsoft Excel. Pearson Prentice Hall.
-   Lind, D. A., Marchal, W. G., & Wathen, S. A. (2012). Statistical techniques in business and economics (15th ed.). McGraw-Hill Irwin.
-   McClave, J. T., Benson, P. G., & Sincich, T. (2011). Statistics for business and economics (11th ed.). Prentice Hall.
-   Montgomery, D. C., Peck, E. A., & Vining, G. G. (2012). Introduction to linear regression analysis. John Wiley & Sons.
-   Moore, D. S., McCabe, G. P., & Craig, B. A. (2009). Introduction to the Practice of Statistics. W.H. Freeman and Co.
-   Rosner, B. (2015). Fundamentals of biostatistics. Cengage Learning.
-   Siegel, S., & Castellan, N. J. (1988). Nonparametric statistics for the behavioral sciences. McGraw-Hill.
-   Tabachnick, B. G., & Fidell, L. S. (2013). Using multivariate statistics. Pearson.
-   Triola, M. F. (2018). Elementary Statistics. Pearson Education.
-   Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.
-   Vasishth, S., & Broe, M. (2011). The foundations of statistics: A simulation-based approach. Springer Science & Business Media.
-   Winer, B. J., Brown, D. R., & Michels, K. M. (1991). Statistical principles in experimental design. McGraw-Hill.
